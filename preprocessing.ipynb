{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Pre-processing and Feature Extraction\n","\n","This notebook contains all necessary code to replicate the feature extraction and final csv files that were used in the statistical analyses.\n","\n","The K-Emophone dataset should be present in the same folder as this notebook. <br>\n","Due to the relevant csv files being rather large and time-consuming to process (20-30 minutes), the final Dataframes used for analysis can be found in the zip-file as well.\n","\n","Fixed Effects Regression Model: total_df.csv\n","\n","Multiple Linear Regression Model: total_df_weekly.csv\n","\n","### Sidenote\n","Some markdowns explaining the features can also be found in the final paper. To provide clarity to the code, we decided to add some explanations here as well."],"metadata":{"id":"Br_e0OjyHjIv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"61V7VGcYl__w"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from datetime import datetime\n","from functools import reduce"]},{"cell_type":"markdown","source":["### Convert timestamps\n","The data contains Unix timestamps in miliseconds. We convert these to local time using date.time, which is later stored in column 'responseTime'. Out-of-bounds values are caught and dropped."],"metadata":{"id":"09-kniKDmOTg"}},{"cell_type":"code","source":["# safe convert for timestamps\n","def safe_convert(timestamp):\n","    try:\n","        # timestamp unit in milliseconds\n","        dt = pd.to_datetime(timestamp, unit='ms', utc=True)\n","        # convert from UTC to Asia/Tokyo timezone\n","        return dt.tz_convert('Asia/Tokyo')\n","    except pd.errors.OutOfBoundsDatetime:\n","        return pd.NaT  # return Not a Time for out-of-bound values"],"metadata":{"id":"GFNfFVAomCFi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading data\n","\n","The initial dataset is structured to contain measurement data as subfolders per person. Because we want to access the data per topic for all individuals instead of the other way around, we traverse through the directory structure and its subfolders. We then initialise a list that collects the dataframes we are interested in, with each dataframe corresponding to each participant.\n","\n","This is not needed for ESMResponse and userInfo this is not needed, since they already contain all participants in the data."],"metadata":{"id":"MURwG-XrxjzZ"}},{"cell_type":"code","source":["# access dataset\n","base_dir = 'k-emophone'\n","\n","# lists to hold the dataframes\n","calories = []\n","screenEvent = []\n","activity = []\n","\n","for root, dirs, files in os.walk(base_dir):\n","        if root[-3] == 'P':\n","                p_val = root[-3:]\n","                for file in files:\n","                        # access relevant files\n","                        if file in ['Calorie.csv', 'ScreenEvent.csv', 'ActivityEvent.csv']:\n","                                file_path = os.path.join(root, file)\n","                                df = pd.read_csv(file_path)\n","                                df['timestamp'] = df['timestamp'].apply(safe_convert)\n","                                df['pcode'] = p_val # track pcode of each participant and add to all dataframes\n","                                if file == 'Calorie.csv':\n","                                        calories.append(df)\n","                                elif file == 'ScreenEvent.csv':\n","                                        screenEvent.append(df)\n","                                elif file == 'ActivityEvent.csv':\n","                                        activity.append(df)\n","\n","ESMResponse = pd.read_csv(os.path.join(base_dir, 'SubjData', 'EsmResponse.csv'))\n","userInfo = pd.read_csv(os.path.join(base_dir, 'SubjData', 'UserInfo.csv'))"],"metadata":{"id":"9MCCW3ixmHi-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Feature Extraction\n","\n","\n","### Stress feature\n","\n","First, date from responseTime is extracted, so the data can be grouped on day. Next, the mean stress score on a 7-point scale is retrieved from ESMresponse, and a daily mean value is taken. A new dataframe is created storing each daily stress score for each participant."],"metadata":{"id":"7FUm4-X3mLrj"}},{"cell_type":"code","source":["# initialise new dataframe\n","Stress = pd.DataFrame()\n","\n","ESMResponse['responseTime'] = ESMResponse['responseTime'].apply(safe_convert) # # apply safe conversion from timestamps to datetime\n","ESMResponse = ESMResponse.dropna(subset=['responseTime']) # only continue processing rows that have a valid 'responseTime'\n","\n","ESMResponse['date'] = ESMResponse['responseTime'].dt.date # extract date from responseTime\n","grouped_df = ESMResponse.groupby(['pcode', 'date'])['stress'].mean().reset_index() # group by person and day, take aggregate stress values\n","Stress = pd.concat([Stress, grouped_df], ignore_index=True) # append to the corresponding dataframe\n","\n","display(Stress)"],"metadata":{"id":"gTwzCuzOmLDt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Screen time feature\n","Using the screenEvent dataframes, we group the data by day and participant, extracting the date from the converted time.\n","\n","The files in screenEvent.csv contain information of screen status per participant. 3 possible screen events have been recorded, namely 'ON', 'UNLOCK' and 'OFF'. ON refers to the screen being turned on whilst still being locked. Therefore, we decided to calculate screen time from the moment when the screen is actually unlocked (UNLOCK event) until the screen in turned off again (OFF event).  \n","Screen time is calculated by summing the miliseconds between Unlock and Off in column 'type'. This is then converted to seconds and stored in a new Dataframe per participant.\n","\n","All data is stored in dataframe 'Screen_time', accounting for each participants' daily screen time."],"metadata":{"id":"89UXB8qKmXt2"}},{"cell_type":"code","source":["# initialize dataframe\n","Screen_time = pd.DataFrame()\n","\n","merged_df = pd.concat(screenEvent)\n","merged_df['timestamp'] = merged_df['timestamp'].apply(safe_convert)\n","merged_df['date'] = merged_df['timestamp'].dt.date\n","\n","grouped_df = merged_df.groupby('pcode') # group by participant\n","\n","# calculate screen time between an 'UNLOCK' and 'LOCK'\n","def calculate_screentime(person, df):\n","\n","    screen_times = []\n","    day_group = df.groupby('date')\n","\n","    # loop over days\n","    for _, day_df in day_group:\n","        screen_time = 0\n","\n","        unlock_time = None\n","        for _, row in day_df.iterrows():\n","            if row['type'] == 'UNLOCK':\n","                unlock_time = row['time']\n","            elif row['type'] == 'OFF' and unlock_time is not None:\n","                off_time = row['time']\n","                screen_duration = (off_time - unlock_time).total_seconds() # calculate screen time in seconds\n","                screen_time += screen_duration # add to the total screen time for that day\n","                unlock_time = None  # reset unlock_time\n","\n","        screen_times.append(screen_time)\n","\n","    screentime_df = pd.DataFrame(np.array([df['date'].unique(), screen_times, [person]*len(screen_times)]).T, columns=['date', 'screentime', 'pcode']) # create dataframe\n","\n","    return screentime_df\n","\n","\n","screen_times = []\n","\n","for person, group in grouped_df:\n","    screen_times.append(calculate_screentime(person, group))\n","\n","Screen_time = pd.concat(screen_times)\n","display(Screen_time)"],"metadata":{"id":"11hEqDqnmW01"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Activity Event feature\n","Below, we constructed another feature for determining physical activity. Leveraging the activityEvent.csv, which records a confidence level (0-1), ideally, every 15 miliseconds about the activity state of someone's mobile.\n","\n","First, we create a new dataframe to store the information that we are interested in. For each row (a mobile entry) in each dataframe (corresponding to a participant), we record whether someone is active or not. For this we assume that when the mobile is above 80% sure someone is either walking, running or biking, someone likely is. We assume for missing entries that someone is not active. Additionally, this feature only allows us to record activity when someone carries their phone (which is also not guaranteed).\n","\n","Total minutes spent in a period of likely activity is stored under 'active_min', reflecting the daily minutes spent either walking, running or biking. We then categorise a day as inactive' when someone's total daily minutes is < 21.42, if >21.42 and <42.8 'moderate', if >42.8 'highly active', according to WHO guidelines. (https://www.who.int/news-room/fact-sheets/detail/physical-activity)\n","\n"],"metadata":{"id":"3MiVb33OwXLi"}},{"cell_type":"code","source":["# detect changes in active status and return amount of active minutes\n","def calculate_active_minutes(df):\n","    active_columns = ['confidenceRunning', 'confidenceOnBicycle', 'confidenceOnFoot', 'confidenceWalking']\n","    df['isActive'] = df[active_columns].max(axis=1) > 0.8\n","\n","    df['activityChange'] = df['isActive'].diff().ne(0).cumsum()\n","\n","    # filter only active periods and calculate the duration\n","    active_periods = df[df['isActive']].groupby('activityChange').agg(start_time=('timestamp', 'min'), end_time=('timestamp', 'max'))\n","    active_periods['duration_minutes'] = (active_periods['end_time'] - active_periods['start_time']).dt.total_seconds() / 60 # convert to seconds\n","\n","    # group by day\n","    active_periods['date'] = active_periods['start_time'].dt.floor('D').dt.date\n","    daily_active_minutes = active_periods.groupby('date')['duration_minutes'].sum().reset_index()\n","\n","    # classify activity levels based on WHO guidelines\n","    daily_active_minutes['activity_level'] = daily_active_minutes['duration_minutes'].apply(\n","        lambda x: 'inactive' if x < 21.42 else ('moderate' if x < 42.8 else 'highly active')\n","    )\n","\n","    daily_active_minutes.insert(0, 'pcode', df['pcode'][0]) # make sure pcode is added\n","\n","    # rename columns and order them\n","    daily_active_minutes.columns = ['pcode', 'date', 'active_min', 'activity_level']\n","\n","    return daily_active_minutes\n","\n","# Processing each DataFrame in list 'activityEvent'\n","dfs = []\n","total_minutes_list = []\n","for df in activity:\n","    df = df.dropna(subset=['timestamp'])\n","\n","    daily_active_minutes = calculate_active_minutes(df)\n","    total_minutes_list.append(daily_active_minutes)\n","\n","    dfs.append(daily_active_minutes)\n","\n","AE = pd.concat(dfs)"],"metadata":{"id":"YpApasSkmgqe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Calories feature\n","The Calorie Dataframe contains information on how many calories a participant has burned on that day and in total since the beginning of the experiment.\n","\n","We create a new Dataframe where each row corresponds to a specific day per participant. Firstly, we retrieve the amount of calories burned on a specific day by identifying the last entry of that day. Since the original CSV files record calories cumulatively for each day, the value in the last entry provides the total number of calories burned for that day.\n","For total burned calories (caloriesTotal) we can directly use the data from the original Dataframe as the columns correspond perfectly.\n","\n","To determine whether a participant has had an active day or not, we estimate their average resting calorie expenditure (REE), also known as BMR (Basal Metabolic Rate). The equations for men and women respectively are as follows.\n","\n","BMR (male) = ( 13.7516 × weight in kg ) + ( 5.0033 × height in cm ) – ( 6.755 × age in years ) + 66.473 <br>\n","BMR (female) = ( 9.5634 × weight in kg ) + ( 1.8496 × height in cm ) – ( 4.6756 × age in years ) + 655.0955\n","\n","Since we don't know the weight and height of the participants, we will estimate the BMR's for men and women based on the average weight and height of Korean young adults (specification can be found in the final paper).\n",". On the other hand, we do have information on participants' gender and age, allowing us to directly insert this information into the right formula.\n","\n","BMR (male) = ( 13.7516 × 76,5 ) + ( 5.0033 × 174 ) – ( 6.755 × age in years ) + 66.473 <br>\n","BMR (female) = ( 9.5634 × 57 ) + ( 1.8496 × 161 ) – ( 4.6756 × age in years ) + 655.0955\n","\n","See our final paper for more information on the equations above.\n","\n","In order to classify participants' days we apply the following logic rules. If participants burn more than 1.2 times their BMR, the day gets classified as 'moderately active'. If one burns less than that, the day gets classified as 'inactive'. Lastly, if one burns more than 1.55 times their BMR, the day gets classified as 'highly active'.\n","\n","#### Missing values:\n","A problem we ran into is that the participants don't always seem to wear the smartband when they're expected to wear it. The total amount of data entries per day are included. We ignore the days with less then 10.000 data entries, as this data seems unreliable.\n"],"metadata":{"id":"vhRO-XpsmuQi"}},{"cell_type":"code","source":["# calculate BMR for each participant\n","def calculate_bmr(row):\n","    if row['gender'] == 'M':\n","        return round((13.7516 * 76.5) + (5.0033 * 174) - (6.755 * row['age']) + 66.473, 2)\n","    else: # assuming that there's only 2 genders\n","        return round((9.5634 * 57) + (1.8496 * 161) - (4.6756 * row['age']) + 655.0955, 2)\n","\n","# function for determining PAlevel based on the given conditions\n","def determine_palevel(row):\n","    if row['caloriesToday'] < row['BMR'] * 1.2:\n","        return 'inactive'\n","    elif row['BMR'] * 1.2 <= row['caloriesToday'] < row['BMR'] * 1.55:\n","        return 'moderately active'\n","    else:\n","        return 'highly active'\n"],"metadata":{"id":"dB1SY_7mmu_v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize new dataframe\n","Calories = pd.DataFrame()\n","\n","# loop over all participants\n","for df in calories:\n","    df['timestamp'] = df['timestamp'].apply(safe_convert)\n","    df = df.dropna(subset=['timestamp'])\n","    df['date'] = df['timestamp'].dt.date\n","\n","    # track number of datapoints for each day\n","    datapoints_per_day = df.groupby('date').size().reset_index(name='datapointsToday')\n","\n","    # identify the last entry of each day\n","    last_entries = df.groupby('date').tail(1).copy()\n","    last_entries['pcode'] = df['pcode'][0] # add pcode to the last entries\n","\n","    last_entries = last_entries.merge(datapoints_per_day, on='date') # merge dataframes\n","    Calories = pd.concat([Calories, last_entries[['pcode', 'date', 'datapointsToday', 'caloriesToday', 'totalCalories']]], ignore_index=True)  # append to the final DataFrame\n","\n","\n","# sort the dataframe by pcode and date\n","Calories = Calories.sort_values(by=['pcode', 'date']).reset_index(drop=True)"],"metadata":{"id":"mHhT_-k3mw50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# caluclate BMR\n","Calories = Calories.merge(userInfo[['pcode', 'gender', 'age']], on='pcode') # merge with UserInfo to retrieve gender and age\n","\n","Calories = Calories[Calories['datapointsToday'] >= 10000] # filter out rows where datapointsToday is under 10000\n","\n","Calories['BMR'] = Calories.apply(calculate_bmr, axis=1) # call calculate BMR function\n","\n","Calories['caloriesActive'] = Calories['caloriesToday'] - Calories['BMR'] # add column for difference between caloriesToday and BMR\n","\n","Calories['PAlevel'] = Calories.apply(determine_palevel, axis=1) # apply the function to determine the PAlevel for each row"],"metadata":{"id":"Kff6neyFm15D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Combined physical activity feature\n","\n","This feature is a combination of Calories and AE. Both of these features have the column PAlevel included in the DataFrame. This column takes as value either 'inactive', 'moderately active', or 'highly active' depending on the amount of calories burned through activity and how many minutes a participant has been active on a given day. <br>\n","When a participant's activity levels on a given day differ only one level from each other, we will choose the lower activity level in our combined physical activity feature. For example, if Calories classifies a participant's day as 'highly active', but AE classifies the same day as 'moderately active', we will use 'moderately active' as the activity level for the combined physical activity feature. When the activity levels differ with more than one level (i.e. activity level of one feature is 'inactive' and the other one is 'highly active'), we will use the middle ground: 'moderately active'.\n","This way we aim to create a more accurate and powerful feature by using the second feature as a control.\n","\n","If activity levels from both features don't correspond, there is a possibility that the data is inaccurate or incorrectly interpreted. A high level of burned calories could, for example, be due to a participant being larger. Since we don't have information on participants' height and weight and simply used the average for Korean young adults in our calculations, we need to ensure that we interpret the data the right way. Similarly, if a participant has not been wearing their Smartband sufficiently enough for us to draw any conclusions on their Calories data, this way we still have an opportunity to include their physical activity in the feature (if their AE activity level is high enough). The same goes for participants who might not take their phone when exercising, their AE data might not be complete, while their Calories data might indicate that they have been active.  "],"metadata":{"id":"0pfBTPesEUSx"}},{"cell_type":"code","source":["# determine combined activity level\n","def combine_activity_levels(row):\n","    levels = ['inactive', 'moderately active', 'highly active']\n","    level_calories = row['activity_level_calories']\n","    level_ae = row['activity_level_ae']\n","\n","    # retrieve the index of each activity level\n","    index_calories = levels.index(level_calories)\n","    index_ae = levels.index(level_ae)\n","\n","    if abs(index_calories - index_ae) == 1:\n","        return levels[min(index_calories, index_ae)] # choose the lower activity level\n","    elif abs(index_calories - index_ae) >= 2:\n","        return 'moderately active'  # use the middle ground: 'moderately active'\n","    else:\n","        return level_calories # if both are the same level, return that level"],"metadata":{"id":"l8EYnXb5EYaS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# merge DataFrames on participant and date\n","PA = pd.merge(Calories, AE, on=['pcode', 'date'], suffixes=('_calories', '_ae'))\n","\n","# apply combining function to the merged DataFrame\n","PA['activity_level'] = PA.apply(combine_activity_levels, axis=1)\n","PA.rename(columns={'date_calories': 'date'}, inplace=True)\n","\n","PA = PA[['pcode', 'date', 'activity_level']]  # only keep relevant columns"],"metadata":{"id":"RJdaMJ1YEb_d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Missing values\n","Not every participant dataset contains data for the full 7 days of the experiment. To make sure we only include participants with sufficient data, we decided to leave out any participants that have less than 3 days of relevant data.\n","\n","From the displayed dataframes below, we can see that P61 and P22 both have one type of data that only contains 2 days in total. Therefore, we will be removing P61 and P22 from our dataset as a whole."],"metadata":{"id":"Nk7T8azEm41m"}},{"cell_type":"code","source":["# stress levels\n","p_missing_SL = Stress.groupby('pcode').filter(lambda x: len(x) < 3)\n","display(p_missing_SL)\n","\n","# activity event\n","p_missing_AE = daily_active_minutes.groupby('pcode').filter(lambda x: len(x) < 3)\n","display(p_missing_AE)\n","\n","# screen time\n","p_missing_ST = Screen_time.groupby('pcode').filter(lambda x: len(x) < 3)\n","display(p_missing_ST)\n","\n","# calories\n","p_missing_CA = Calories.groupby('pcode').filter(lambda x: len(x) < 3)\n","display(p_missing_CA)"],"metadata":{"id":"UmV1duPTm4Md"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fixed Effects Regression Model: Final Dataframe\n","\n","Here all output from the features is concatenated into one dataframe, being matched on pcode and date."],"metadata":{"id":"68BbZiqGwxPO"}},{"cell_type":"code","source":["# merge dataframes\n","dataframes = [Stress, AE, Screen_time, Calories, PA]\n","total_df = reduce(lambda left, right: pd.merge(left, right, on=['pcode', 'date'], how='outer'), dataframes)\n","\n","total_df = total_df[~total_df['pcode'].isin(['P22', 'P61'])] # delete rows for participant P22 and P61\n","total_df = total_df.sort_values(by=['pcode', 'date'], ascending=[True, True])\n","\n","\n","# length stress: 535\n","# length AE: 534\n","# length stress: 556\n","# length stress: 480\n","\n","# total nans: 113\n","# total nans after deleting P22 and P61: 103"],"metadata":{"id":"qzkV3Pa5wvqc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save df as csv file\n","total_df.to_csv('total_df.csv', index=False)"],"metadata":{"id":"bO5UJspmEwjl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Multiple Linear Regression Model\n","\n","\n","To merge the daily variables into single, weekly variables for each participant, the median is taken for all features.\n"],"metadata":{"id":"IjWDGPS7EtmV"}},{"cell_type":"code","source":["# Depression scores\n","Depression_scores_ = userInfo[['pcode', 'PHQ']]\n","\n","# Stress levels\n","Stress_ = pd.DataFrame(Stress.groupby('pcode')['stress'].median())\n","\n","#Screen time\n","Screen_time['screentime'] = pd.to_numeric(Screen_time['screentime'], errors='coerce') # Ensure screentime is numeric\n","Screen_time_ = pd.DataFrame(Screen_time.groupby('pcode')['screentime'].median())\n","\n","# Acitivity event\n","AE_ = pd.DataFrame(AE.groupby('pcode')['active_min'].median())\n","# classify activity levels based on WHO guidelines\n","AE_['activity_level'] = AE_['active_min'].apply(\n","        lambda x: 'inactive' if x < 21.42 else ('moderately active' if x < 42.8 else 'highly active')\n","    )\n","\n","# Calories\n","Calories_ = pd.DataFrame(Calories.groupby('pcode')[['caloriesToday', 'BMR']].median())\n","Calories_['activity_level'] = Calories_.apply(determine_palevel, axis=1) # apply the function to determine the PAlevel\n","\n","# Physical activity\n","PA_ = pd.merge(Calories_, AE_, on='pcode', suffixes=('_calories', '_ae'))\n","PA_['activity_level'] = PA_.apply(combine_activity_levels, axis=1) # classify activity levels\n","PA_ = PA_[['activity_level']]"],"metadata":{"id":"vksYmi3yEuFi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# merge dataframes\n","dataframes_ = [Stress_, PA_, Calories_, Screen_time_, Depression_scores_]\n","total_df_ = reduce(lambda left, right: pd.merge(left, right, on=['pcode'], how='outer'), dataframes_)\n","\n","\n","total_df_ = total_df_[~total_df_['pcode'].isin(['P22', 'P61'])] # Delete rows for participant P22 and P61\n","total_df_ = total_df_.sort_values(by=['pcode'])"],"metadata":{"id":"NW8gPbLLExZi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save df as csv file\n","total_df_.to_csv('total_df_weekly.csv', index=False)"],"metadata":{"id":"8SagEOHdSdTl"},"execution_count":null,"outputs":[]}]}